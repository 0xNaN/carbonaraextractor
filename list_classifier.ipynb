{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>avg_tag_in_li</th>\n",
       "      <th>depth</th>\n",
       "      <th>number_bold</th>\n",
       "      <th>number_br</th>\n",
       "      <th>number_div</th>\n",
       "      <th>number_img</th>\n",
       "      <th>number_links</th>\n",
       "      <th>number_p</th>\n",
       "      <th>number_relevants</th>\n",
       "      <th>number_row</th>\n",
       "      <th>relevants_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.33</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relevant  avg_tag_in_li  depth  number_bold  number_br  number_div  \\\n",
       "1156       1.0           0.00    1.0          0.0        0.0         0.0   \n",
       "1356       1.0           0.00    2.0          1.0        1.0         0.0   \n",
       "362        0.0           8.33    8.0          0.0        0.0        42.0   \n",
       "1331       1.0           2.00    2.0          0.0        0.0         2.0   \n",
       "1228       1.0           4.00    4.0          3.0        3.0         0.0   \n",
       "\n",
       "      number_img  number_links  number_p  number_relevants  number_row  \\\n",
       "1156         0.0           0.0       0.0               4.0         4.0   \n",
       "1356         0.0           0.0       0.0               2.0         1.0   \n",
       "362          6.0          12.0       0.0               7.0        18.0   \n",
       "1331         0.0           0.0       0.0               1.0         1.0   \n",
       "1228         0.0           0.0       0.0              14.0         4.0   \n",
       "\n",
       "      relevants_ratio  \n",
       "1156             0.24  \n",
       "1356             0.04  \n",
       "362              0.14  \n",
       "1331             0.33  \n",
       "1228             0.35  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.read_csv(\"data/and/list.csv\", sep=\"\\t\")\n",
    "\n",
    "dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['relevant', 'avg_tag_in_li', 'depth', 'number_bold', 'number_br',\n",
       "       'number_div', 'number_img', 'number_links', 'number_p',\n",
       "       'number_relevants', 'number_row', 'relevants_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted features:\n",
      " ['avg_tag_in_li', 'depth', 'number_bold', 'number_br', 'number_div', 'number_img', 'number_links', 'number_p', 'number_relevants', 'number_row', 'relevants_ratio']\n",
      "\n",
      "Example data vector:\n",
      " [20.    7.    0.    0.   42.    7.   21.    0.   26.    7.    0.18]\n",
      "\n",
      "Example label vector:\n",
      " [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Dataset to numpy arrays\n",
    "\n",
    "TARGET_FEATURE = 'relevant'\n",
    "\n",
    "CHOSEN_FEATURES = [\n",
    "    'avg_tag_in_li',\n",
    "    'depth',\n",
    "    'number_bold',\n",
    "    'number_br',\n",
    "    'number_div',\n",
    "    'number_img',\n",
    "    'number_links',\n",
    "    'number_p',\n",
    "    'number_relevants',\n",
    "    'number_row',\n",
    "    'relevants_ratio'\n",
    "]\n",
    "\n",
    "\n",
    "CHOSEN_FEATURES = sorted(CHOSEN_FEATURES) # XXX as convention we sort the features based on their names\n",
    "print (\"Sorted features:\\n\", CHOSEN_FEATURES)\n",
    "\n",
    "if (TARGET_FEATURE in CHOSEN_FEATURES): CHOSEN_FEATURES.remove(TARGET_FEATURE) # ensure we do not use target feature\n",
    "\n",
    "data  = np.ndarray((len(dataset), len(CHOSEN_FEATURES)), np.float32)\n",
    "label = np.ndarray((len(dataset), 2), np.float32)\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    label[i] = np.zeros(2)\n",
    "    label[i][int(row[TARGET_FEATURE])] = 1\n",
    "\n",
    "    data[i]  = np.fromiter([row[feature] for feature in CHOSEN_FEATURES], np.float32)\n",
    "    \n",
    "    \n",
    "print (\"\\nExample data vector:\\n\", data[0])\n",
    "print (\"\\nExample label vector:\\n\", label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "def list_classifier(neuron, activation, input_shape=()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron, input_shape=input_shape, \n",
    "                            activation=activation\n",
    "                            ))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 32, 8, 'tanh'), (50, 32, 16, 'tanh'), (50, 32, 32, 'tanh')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train & test\n",
    "import itertools\n",
    "\n",
    "idx = np.random.permutation(len(data))\n",
    "data, label = data[idx], label[idx]\n",
    "\n",
    "loo = KFold(4)\n",
    "\n",
    "val_percent = 0.3\n",
    "epochs = [50]\n",
    "batch_size = [32]\n",
    "neurons = [8, 16, 32]\n",
    "activation = ['tanh']\n",
    "\n",
    "hyperparams = list(itertools.product(epochs, batch_size, neurons, activation))\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: early stopping\n",
      "[0.22408606601420697, 0.920993229336448]\n",
      "Epoch 00042: early stopping\n",
      "[0.1678971190081224, 0.9345372464533046]\n",
      "Epoch 00028: early stopping\n",
      "[0.15574721203130834, 0.9502262446135957]\n",
      "Epoch 00036: early stopping\n",
      "[0.1807196961538824, 0.9276018102244554]\n",
      "\n",
      "\n",
      "Training on 1328 samples\n",
      "Testing on 442 samples, with 132.6 of validation\n",
      "\n",
      "\n",
      ">>(50, 32, 8, tanh)  - loss: 0.18211252330188002, acc: 0.933339632656951 (std: 0.012542570488791229)\n",
      "Epoch 00026: early stopping\n",
      "[0.17830832452254694, 0.9390519187358917]\n",
      "Epoch 00032: early stopping\n",
      "[0.16707095490352294, 0.9458239281688802]\n",
      "Epoch 00023: early stopping\n",
      "[0.16739055750326873, 0.9524886880525097]\n",
      "Epoch 00018: early stopping\n",
      "[0.16129552009957948, 0.9389140255310956]\n",
      "\n",
      "\n",
      "Training on 1328 samples\n",
      "Testing on 442 samples, with 132.6 of validation\n",
      "\n",
      "\n",
      ">>(50, 32, 16, tanh)  - loss: 0.16851633925722953, acc: 0.9440696401220943 (std: 0.006473427333497107)\n",
      "Epoch 00016: early stopping\n",
      "[0.1739914400616564, 0.9390519187358917]\n",
      "Epoch 00011: early stopping\n",
      "[0.171853865713352, 0.9390519191395348]\n",
      "Epoch 00016: early stopping\n",
      "[0.14771924933157354, 0.9615384599202359]\n",
      "Epoch 00016: early stopping\n",
      "[0.1533801758586012, 0.9343891405411975]\n",
      "\n",
      "\n",
      "Training on 1328 samples\n",
      "Testing on 442 samples, with 132.6 of validation\n",
      "\n",
      "\n",
      ">>(50, 32, 32, tanh)  - loss: 0.16173618274129578, acc: 0.943507859584215 (std: 0.012219716302668415)\n"
     ]
    }
   ],
   "source": [
    "early_stop_val_acc = EarlyStopping(monitor = \"val_loss\", verbose=True, mode='auto')\n",
    "\n",
    "for epoch, batch, neuron, activation in hyperparams:\n",
    "\n",
    "    res = []\n",
    "    for (i, (train_index, test_index)) in enumerate(loo.split(data)):\n",
    "        \n",
    "        model = list_classifier(neuron, activation, input_shape=(len(CHOSEN_FEATURES),))\n",
    "        \n",
    "        data_train,   data_test =  data[train_index],  data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        \n",
    "        \n",
    "        t = model.fit(data_train, label_train, epochs=epoch, batch_size=batch, shuffle=True, verbose=False,\n",
    "                     validation_split=val_percent, callbacks=[early_stop_val_acc])\n",
    "        \n",
    "        r = model.evaluate(data_test, label_test, verbose=False)\n",
    "        print (r)\n",
    "        res.append(r)\n",
    "        \n",
    "    loss = statistics.mean(list(map(lambda x: x[0], res)))\n",
    "    acc  = statistics.mean(list(map(lambda x: x[1], res)))\n",
    "    acc_std = statistics.stdev(list(map(lambda x: x[1], res)))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print (\"Training on {} samples\".format(len(data_train)))\n",
    "    print (\"Testing on {} samples, with {} of validation\".format(len(data_test), \n",
    "                                                                 len(data_test) * val_percent))\n",
    "    \n",
    "    print(\"\\n\\n>>({}, {}, {}, {})  - loss: {}, acc: {} (std: {})\".format(epoch, batch, neuron, activation,\n",
    "                                                                   loss, acc, acc_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(data_train)\n",
    "\n",
    "yes = []\n",
    "no = []\n",
    "\n",
    "for i, l in enumerate(label_train):\n",
    "    if l[0] == 1:\n",
    "        no.append(predicted[i][0])\n",
    "    else:\n",
    "        yes.append(predicted[i][1])\n",
    "        \n",
    "yes = list(map(float, yes)) \n",
    "no  = list(map(float, no)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES: mean 0.89067799965481 std 0.10966267227792012\n",
      "NO: mean 0.9057175528466611 std 0.2374656861714324\n"
     ]
    }
   ],
   "source": [
    "print (\"YES: mean {} std {}\".format(statistics.mean(yes), statistics.stdev(yes)))\n",
    "print (\"NO: mean {} std {}\".format(statistics.mean(no), statistics.stdev(no)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the best hyperparameters from the results above\n",
    "best_epochs = int(statistics.mean([16, 11, 16, 16]))\n",
    "best_batch_size = 32\n",
    "best_neurons = 32\n",
    "best_activation = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "1770/1770 [==============================] - 0s 278us/step - loss: 0.3269 - acc: 0.8763\n",
      "Epoch 2/14\n",
      "1770/1770 [==============================] - 0s 106us/step - loss: 0.2240 - acc: 0.9232\n",
      "Epoch 3/14\n",
      "1770/1770 [==============================] - 0s 107us/step - loss: 0.1938 - acc: 0.9311\n",
      "Epoch 4/14\n",
      "1770/1770 [==============================] - 0s 131us/step - loss: 0.1772 - acc: 0.9367\n",
      "Epoch 5/14\n",
      "1770/1770 [==============================] - 0s 112us/step - loss: 0.1687 - acc: 0.9379\n",
      "Epoch 6/14\n",
      "1770/1770 [==============================] - 0s 110us/step - loss: 0.1617 - acc: 0.9379\n",
      "Epoch 7/14\n",
      "1770/1770 [==============================] - 0s 103us/step - loss: 0.1583 - acc: 0.9395\n",
      "Epoch 8/14\n",
      "1770/1770 [==============================] - 0s 107us/step - loss: 0.1536 - acc: 0.9401\n",
      "Epoch 9/14\n",
      "1770/1770 [==============================] - 0s 108us/step - loss: 0.1498 - acc: 0.9424\n",
      "Epoch 10/14\n",
      "1770/1770 [==============================] - 0s 110us/step - loss: 0.1462 - acc: 0.9458\n",
      "Epoch 11/14\n",
      "1770/1770 [==============================] - 0s 101us/step - loss: 0.1445 - acc: 0.9446\n",
      "Epoch 12/14\n",
      "1770/1770 [==============================] - 0s 107us/step - loss: 0.1412 - acc: 0.9463\n",
      "Epoch 13/14\n",
      "1770/1770 [==============================] - 0s 106us/step - loss: 0.1379 - acc: 0.9458\n",
      "Epoch 14/14\n",
      "1770/1770 [==============================] - 0s 106us/step - loss: 0.1360 - acc: 0.9492\n"
     ]
    }
   ],
   "source": [
    "## retrain the model on the whole dataset and save it\n",
    "model = list_classifier(best_neurons, best_activation, input_shape=(len(CHOSEN_FEATURES),))\n",
    "\n",
    "model.fit(data, label, epochs=best_epochs, \n",
    "                       batch_size=best_epochs, \n",
    "                       shuffle=True, verbose=True)\n",
    "\n",
    "model.save(\"models/list_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
