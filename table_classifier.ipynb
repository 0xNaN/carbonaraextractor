{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>depth</th>\n",
       "      <th>number_bold</th>\n",
       "      <th>number_br</th>\n",
       "      <th>number_div</th>\n",
       "      <th>number_img</th>\n",
       "      <th>number_li</th>\n",
       "      <th>number_links</th>\n",
       "      <th>number_p</th>\n",
       "      <th>number_relevants</th>\n",
       "      <th>number_td</th>\n",
       "      <th>number_th</th>\n",
       "      <th>number_tr</th>\n",
       "      <th>relevants_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevant  depth  number_bold  number_br  number_div  number_img  number_li  \\\n",
       "0         0      7            0          0          14           6          0   \n",
       "1         0      3            1          0           0           0          0   \n",
       "2         0      5           15         60           0           0          0   \n",
       "3         1      3            0          0           0           0          0   \n",
       "4         1      4            7          0           0           0          0   \n",
       "\n",
       "   number_links  number_p  number_relevants  number_td  number_th  number_tr  \\\n",
       "0            20         0                80         86         19         16   \n",
       "1             0         0                 2          2          1          2   \n",
       "2             0         0               142          5          0          1   \n",
       "3             0         0                60         54          0         27   \n",
       "4             0         0                46         53         53         53   \n",
       "\n",
       "   relevants_ratio  \n",
       "0             0.48  \n",
       "1             0.33  \n",
       "2             0.42  \n",
       "3             0.65  \n",
       "4             0.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.read_csv(\"data/table2.csv\", sep=\"\\t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analisi features\n",
    "Addestrando una rete (epochs:5, neurons:16, batch_size:16, activation:'tanh')\n",
    "    \n",
    "con KFold(8), utilizzando **una** sola feature si Ã¨ ottenuto\n",
    "\n",
    "       1. depth                loss: 0.338, acc: 0.068 (std: 0.137)\n",
    "       2. number_links         loss: 0.293, acc: 0.437 (std: 0.350)\n",
    "       3. number_relevants     loss: 0.303, acc: 0.240 (std: 0.216)\n",
    "       4. number_td            loss: 0.289, acc: 0.440 (std: 0.262)\n",
    "       5. number_th            loss: 0.302, acc: 0.292 (std: 0.305)\n",
    "       6. number_tr            loss: 0.287, acc: 0.450 (std: 0.254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number_bold', 'number_img', 'number_links', 'number_relevants', 'number_td', 'number_tr', 'relevants_ratio']\n",
      "[ 0.    6.   20.   80.   86.   16.    0.48]\n"
     ]
    }
   ],
   "source": [
    "# Dataset to numpy arrays\n",
    "\n",
    "# choose the target feature and the features to train on\n",
    "TARGET_FEATURE = 'relevant'\n",
    "CHOSEN_FEATURES = ['number_img', 'number_td', 'number_tr', 'number_relevants', 'number_links', 'number_bold', 'relevants_ratio']\n",
    "#CHOSEN_FEATURES = [\"relevants_ratio\"]\n",
    "\n",
    "CHOSEN_FEATURES = sorted(CHOSEN_FEATURES) # XXX as convention we sort the features based on their names\n",
    "print(CHOSEN_FEATURES)\n",
    "\n",
    "if (TARGET_FEATURE in CHOSEN_FEATURES): CHOSEN_FEATURES.remove(TARGET_FEATURE) # ensure we do not use target feature\n",
    "\n",
    "data  = np.ndarray((len(dataset), len(CHOSEN_FEATURES)), np.float32)\n",
    "label = np.ndarray((len(dataset), 2), np.float32)\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    label[i] = np.zeros(2)\n",
    "    label[i][int(row[TARGET_FEATURE])] = 1\n",
    "\n",
    "    data[i]  = np.fromiter([row[feature] for feature in CHOSEN_FEATURES], np.float32)\n",
    "    \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "def table_classifier(neuron, activation, input_shape=()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron, input_shape=input_shape, activation=activation))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 8, 32, 'tanh'),\n",
       " (30, 8, 32, 'relu'),\n",
       " (30, 8, 32, 'relu'),\n",
       " (30, 8, 64, 'tanh'),\n",
       " (30, 8, 64, 'relu'),\n",
       " (30, 8, 64, 'relu')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train & test\n",
    "import itertools\n",
    "\n",
    "loo = KFold(6)\n",
    "\n",
    "epochs = [30]\n",
    "batch_size = [8]\n",
    "neurons = [32, 64]\n",
    "activation = ['tanh', 'relu', 'relu']\n",
    "\n",
    "hyperparams = list(itertools.product(epochs, batch_size, neurons, activation))\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00026: early stopping\n",
      "[0.10324679405474271, 0.9704918032786886]\n",
      "Epoch 00025: early stopping\n",
      "[0.1695456682047883, 0.9344262295081968]\n",
      "Epoch 00026: early stopping\n",
      "[0.13122763430974524, 0.9573770491803278]\n",
      "Epoch 00026: early stopping\n",
      "[0.16870917440926442, 0.9475409836065574]\n",
      "Epoch 00021: early stopping\n",
      "[0.10374179953023006, 0.9572368421052632]\n",
      "Epoch 00027: early stopping\n",
      "[0.11156018686137702, 0.9539473684210527]\n",
      "(30, 8, 32, tanh)  - loss: 0.1313385428950246, acc: 0.9535033793500144 (std: 0.011976940720095577)\n",
      "\n",
      "Epoch 00018: early stopping\n",
      "[0.14467889730070457, 0.9245901645207014]\n",
      "Epoch 00021: early stopping\n",
      "[0.17506640809969823, 0.9311475411790316]\n",
      "Epoch 00027: early stopping\n",
      "[0.14218943831739855, 0.9180327868852459]\n",
      "Epoch 00023: early stopping\n",
      "[0.1795263596978344, 0.9344262297036218]\n",
      "Epoch 00015: early stopping\n",
      "[0.1285712193501623, 0.9407894736842105]\n",
      "Epoch 00018: early stopping\n",
      "[0.13427417078300527, 0.9342105263157895]\n",
      "(30, 8, 32, relu)  - loss: 0.15071774892480055, acc: 0.9305327870481002 (std: 0.008072508554350427)\n",
      "\n",
      "Epoch 00025: early stopping\n",
      "[0.12159400750867656, 0.957377049375753]\n",
      "Epoch 00014: early stopping\n",
      "[0.1667789699357064, 0.9377049180327869]\n",
      "Epoch 00021: early stopping\n",
      "[0.1365528523066982, 0.9311475409836065]\n",
      "Epoch 00017: early stopping\n",
      "[0.1762920690364525, 0.9409836067528021]\n",
      "Epoch 00015: early stopping\n",
      "[0.13005863247733368, 0.9539473684210527]\n",
      "Epoch 00016: early stopping\n",
      "[0.1482759847452766, 0.9407894736842105]\n",
      "(30, 8, 32, relu)  - loss: 0.14659208600169066, acc: 0.9436583262083686 (std: 0.01001414010102001)\n",
      "\n",
      "Epoch 00020: early stopping\n",
      "[0.09340236143010562, 0.9606557379003431]\n",
      "Epoch 00023: early stopping\n",
      "[0.17121643169981535, 0.9278688524590164]\n",
      "Epoch 00016: early stopping\n",
      "[0.12831170964436453, 0.9508196721311475]\n",
      "Epoch 00019: early stopping\n",
      "[0.15790544015462282, 0.9245901643252764]\n",
      "Epoch 00014: early stopping\n",
      "[0.11821320927456806, 0.9407894736842105]\n",
      "Epoch 00023: early stopping\n",
      "[0.1246293194984135, 0.9407894736842105]\n",
      "(30, 8, 64, tanh)  - loss: 0.1322797452836483, acc: 0.9409188956973674 (std: 0.013598035968452028)\n",
      "\n",
      "Epoch 00018: early stopping\n",
      "[0.12128852224740826, 0.9639344262295082]\n",
      "Epoch 00017: early stopping\n",
      "[0.1473672591516229, 0.9278688524590164]\n",
      "Epoch 00019: early stopping\n",
      "[0.1388077882225396, 0.9245901639344263]\n",
      "Epoch 00017: early stopping\n",
      "[0.227690141806837, 0.8983606561285551]\n",
      "Epoch 00013: early stopping\n",
      "[0.11915943261824156, 0.9473684210526315]\n",
      "Epoch 00015: early stopping\n",
      "[0.16529064184348835, 0.930921052631579]\n",
      "(30, 8, 64, relu)  - loss: 0.15326729764835628, acc: 0.9321739287392861 (std: 0.022183579833155713)\n",
      "\n",
      "Epoch 00023: early stopping\n",
      "[0.14235314729272341, 0.9409836069482271]\n",
      "Epoch 00025: early stopping\n",
      "[0.1568416508250549, 0.940983606557377]\n",
      "Epoch 00012: early stopping\n",
      "[0.1497944376996306, 0.9081967213114754]\n",
      "Epoch 00014: early stopping\n",
      "[0.17396270412890638, 0.9344262297036218]\n",
      "Epoch 00019: early stopping\n",
      "[0.10779201043279547, 0.9605263157894737]\n",
      "Epoch 00019: early stopping\n",
      "[0.1570838630983704, 0.9243421052631579]\n",
      "(30, 8, 64, relu)  - loss: 0.14797130224624686, acc: 0.9349097642622222 (std: 0.01763795182796112)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop_val_acc = EarlyStopping(monitor = \"val_loss\", min_delta=0.005, patience=5, verbose=True)\n",
    "\n",
    "for epoch, batch, neuron, activation in hyperparams:\n",
    "\n",
    "    res = []\n",
    "    for (i, (train_index, test_index)) in enumerate(loo.split(data)):\n",
    "        \n",
    "        model = table_classifier(neuron, activation, input_shape=(len(CHOSEN_FEATURES),))\n",
    "        \n",
    "        data_train,   data_test =  data[train_index],  data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        \n",
    "        \n",
    "        t = model.fit(data_train, label_train, epochs=epoch, batch_size=batch, shuffle=True, verbose=False,\n",
    "                 validation_split=0.3, callbacks=[early_stop_val_acc])\n",
    "        \n",
    "        r = model.evaluate(data_test, label_test, verbose=False)\n",
    "        print(r)\n",
    "        res.append(r)\n",
    "        \n",
    "    loss = statistics.mean(list(map(lambda x: x[0], res)))\n",
    "    acc  = statistics.mean(list(map(lambda x: x[1], res)))\n",
    "    acc_std = statistics.stdev(list(map(lambda x: x[1], res)))\n",
    "    \n",
    "    print(\"({}, {}, {}, {})  - loss: {}, acc: {} (std: {})\".format(epoch, batch, neuron, activation,\n",
    "                                                                   loss, acc, acc_std))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the best hyperparameters from the results above\n",
    "best_epochs = 30    \n",
    "best_batch_size = 8\n",
    "best_neurons = 32\n",
    "best_activation = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1828/1828 [==============================] - 2s 1ms/step - loss: 1.0709 - acc: 0.3468\n",
      "Epoch 2/30\n",
      "1828/1828 [==============================] - 0s 77us/step - loss: 0.5372 - acc: 0.7117\n",
      "Epoch 3/30\n",
      "1828/1828 [==============================] - 0s 77us/step - loss: 0.3324 - acc: 0.8911\n",
      "Epoch 4/30\n",
      "1828/1828 [==============================] - 0s 119us/step - loss: 0.2686 - acc: 0.9141\n",
      "Epoch 5/30\n",
      "1828/1828 [==============================] - 0s 90us/step - loss: 0.2322 - acc: 0.9272\n",
      "Epoch 6/30\n",
      "1828/1828 [==============================] - 0s 60us/step - loss: 0.2079 - acc: 0.9256\n",
      "Epoch 7/30\n",
      "1828/1828 [==============================] - 0s 61us/step - loss: 0.1905 - acc: 0.9272\n",
      "Epoch 8/30\n",
      "1828/1828 [==============================] - 0s 61us/step - loss: 0.1790 - acc: 0.9338\n",
      "Epoch 9/30\n",
      "1828/1828 [==============================] - 0s 60us/step - loss: 0.1710 - acc: 0.9327\n",
      "Epoch 10/30\n",
      "1828/1828 [==============================] - 0s 57us/step - loss: 0.1655 - acc: 0.9409\n",
      "Epoch 11/30\n",
      "1828/1828 [==============================] - 0s 57us/step - loss: 0.1609 - acc: 0.9404\n",
      "Epoch 12/30\n",
      "1828/1828 [==============================] - 0s 60us/step - loss: 0.1556 - acc: 0.9409\n",
      "Epoch 13/30\n",
      "1828/1828 [==============================] - 0s 64us/step - loss: 0.1527 - acc: 0.9409\n",
      "Epoch 14/30\n",
      "1828/1828 [==============================] - 0s 65us/step - loss: 0.1507 - acc: 0.9404\n",
      "Epoch 15/30\n",
      "1828/1828 [==============================] - 0s 59us/step - loss: 0.1466 - acc: 0.9415\n",
      "Epoch 16/30\n",
      "1828/1828 [==============================] - 0s 67us/step - loss: 0.1458 - acc: 0.9442\n",
      "Epoch 17/30\n",
      "1828/1828 [==============================] - 0s 60us/step - loss: 0.1429 - acc: 0.9415\n",
      "Epoch 18/30\n",
      "1828/1828 [==============================] - 0s 59us/step - loss: 0.1405 - acc: 0.9469\n",
      "Epoch 19/30\n",
      "1828/1828 [==============================] - 0s 67us/step - loss: 0.1389 - acc: 0.9464\n",
      "Epoch 20/30\n",
      "1828/1828 [==============================] - 0s 65us/step - loss: 0.1368 - acc: 0.9453\n",
      "Epoch 21/30\n",
      "1828/1828 [==============================] - 0s 66us/step - loss: 0.1351 - acc: 0.9486\n",
      "Epoch 22/30\n",
      "1828/1828 [==============================] - 0s 63us/step - loss: 0.1347 - acc: 0.9409\n",
      "Epoch 23/30\n",
      "1828/1828 [==============================] - 0s 66us/step - loss: 0.1320 - acc: 0.9442\n",
      "Epoch 24/30\n",
      "1828/1828 [==============================] - 0s 64us/step - loss: 0.1310 - acc: 0.9447\n",
      "Epoch 25/30\n",
      "1828/1828 [==============================] - 0s 61us/step - loss: 0.1294 - acc: 0.9453\n",
      "Epoch 26/30\n",
      "1828/1828 [==============================] - 0s 79us/step - loss: 0.1276 - acc: 0.9431\n",
      "Epoch 27/30\n",
      "1828/1828 [==============================] - 0s 70us/step - loss: 0.1269 - acc: 0.9480\n",
      "Epoch 28/30\n",
      "1828/1828 [==============================] - 0s 72us/step - loss: 0.1262 - acc: 0.9464\n",
      "Epoch 29/30\n",
      "1828/1828 [==============================] - 0s 74us/step - loss: 0.1241 - acc: 0.9464\n",
      "Epoch 30/30\n",
      "1828/1828 [==============================] - 0s 60us/step - loss: 0.1228 - acc: 0.9458\n"
     ]
    }
   ],
   "source": [
    "## retrain the model on the whole dataset and save it\n",
    "model = table_classifier(best_neurons, best_activation, input_shape=(len(CHOSEN_FEATURES),))\n",
    "\n",
    "model.fit(data, label, epochs=best_epochs, \n",
    "                       batch_size=best_epochs, \n",
    "                       shuffle=True, verbose=True)\n",
    "\n",
    "model.save(\"models/table_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
